
资料：[Hello! · Transformers快速入门](https://transformers.run/)

注意力
- [scaled dot-product attention](./scaled_dot_product_attention.py)
- [multi-head attention](./multi_head_attention.py)
